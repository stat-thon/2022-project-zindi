{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import timm\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_net_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 41):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your own directory\n",
    "\n",
    "file_dir = 'D:/thon/DL/zindi'\n",
    "img_dir = file_dir + '/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(file_dir + \"/Train.csv\")\n",
    "test_df = pd.read_csv(file_dir + '/Test.csv')\n",
    "train_labels = train_df[\"Label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "train_labels = [label_unique[k] for k in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'D:/thon/DL/zindi/Images/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\thon\\DL\\zindi_efficientnetb0.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39m### You don't need to do it twice\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=2'>3</a>\u001b[0m \u001b[39m# Generate new 'train', 'test' image folders in your file_directory\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=4'>5</a>\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(img_dir \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/train\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=5'>6</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(img_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/test\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_efficientnetb0.ipynb#ch0000004?line=7'>8</a>\u001b[0m \u001b[39m# Set train and test directories\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=222'>223</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=223'>224</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=224'>225</a>\u001b[0m     mkdir(name, mode)\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=225'>226</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=226'>227</a>\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=227'>228</a>\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/sujin/anaconda3/envs/tf/lib/os.py?line=228'>229</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'D:/thon/DL/zindi/Images/train'"
     ]
    }
   ],
   "source": [
    "### You don't need to do it twice\n",
    "\n",
    "# Generate new 'train', 'test' image folders in your file_directory\n",
    "\n",
    "os.makedirs(img_dir + '/train')\n",
    "os.makedirs(img_dir + '/test')\n",
    "\n",
    "# Set train and test directories\n",
    "\n",
    "train_dir = img_dir + '/train'\n",
    "test_dir = img_dir + '/test'\n",
    "\n",
    "### To move files according to train.csv and test.csv's 'Image_id' column\n",
    "\n",
    "train_file_name = train_df[\"Image_id\"]\n",
    "train_file_label = train_df[\"Label\"]\n",
    "test_file_name = test_df[\"Image_id\"]\n",
    "\n",
    "# Move train images from 'Images' folder to 'train' folder\n",
    "\n",
    "for i in train_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = train_dir\n",
    "    shutil.move(file_source, file_destination)\n",
    "\n",
    "\n",
    "# Move test images from 'Images' folder to 'test' folder\n",
    "\n",
    "for i in test_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = test_dir\n",
    "    shutil.move(file_source, file_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = img_dir + '/train'\n",
    "test_dir = img_dir + '/test'\n",
    "\n",
    "train_jpg = sorted(glob(train_dir +'/*.jpg'))\n",
    "test_jpg = sorted(glob(test_dir + '/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224 # input size for efficientnet_b0 : 224 * 224\n",
    "\n",
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1619/1619 [00:27<00:00, 58.38it/s]\n",
      "100%|██████████| 1080/1080 [00:18<00:00, 58.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_imgs = [img_load(m) for m in tqdm(train_jpg)]\n",
    "test_imgs = [img_load(n) for n in tqdm(test_jpg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_dataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, mode='train'):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.mode=mode\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_paths[idx]\n",
    "        if self.mode=='train':\n",
    "            augmentation = random.randint(0,2)\n",
    "            if augmentation==1:\n",
    "                img = img[::-1].copy()\n",
    "            elif augmentation==2:\n",
    "                img = img[:,::-1].copy()\n",
    "        img = transforms.ToTensor()(img)\n",
    "        if self.mode=='test':\n",
    "            pass\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "# Train\n",
    "train_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test\n",
    "test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/25    time : 22s/533s\n",
      "TRAIN    loss : 0.46602    AUC : 0.90706\n",
      "epoch : 2/25    time : 14s/317s\n",
      "TRAIN    loss : 0.06969    AUC : 0.98085\n",
      "epoch : 3/25    time : 14s/303s\n",
      "TRAIN    loss : 0.03110    AUC : 0.98888\n",
      "epoch : 4/25    time : 14s/294s\n",
      "TRAIN    loss : 0.00838    AUC : 0.99753\n",
      "epoch : 5/25    time : 14s/279s\n",
      "TRAIN    loss : 0.04013    AUC : 0.98888\n",
      "epoch : 6/25    time : 14s/265s\n",
      "TRAIN    loss : 0.01141    AUC : 0.99629\n",
      "epoch : 7/25    time : 14s/251s\n",
      "TRAIN    loss : 0.01423    AUC : 0.99506\n",
      "epoch : 8/25    time : 14s/239s\n",
      "TRAIN    loss : 0.01366    AUC : 0.99444\n",
      "epoch : 9/25    time : 14s/223s\n",
      "TRAIN    loss : 0.00562    AUC : 0.99815\n",
      "epoch : 10/25    time : 14s/212s\n",
      "TRAIN    loss : 0.00085    AUC : 1.00000\n",
      "epoch : 11/25    time : 14s/197s\n",
      "TRAIN    loss : 0.00046    AUC : 1.00000\n",
      "epoch : 12/25    time : 14s/182s\n",
      "TRAIN    loss : 0.00045    AUC : 1.00000\n",
      "epoch : 13/25    time : 14s/168s\n",
      "TRAIN    loss : 0.00031    AUC : 1.00000\n",
      "epoch : 14/25    time : 14s/154s\n",
      "TRAIN    loss : 0.00024    AUC : 1.00000\n",
      "epoch : 15/25    time : 14s/140s\n",
      "TRAIN    loss : 0.00003    AUC : 1.00000\n",
      "epoch : 16/25    time : 14s/126s\n",
      "TRAIN    loss : 0.00004    AUC : 1.00000\n",
      "epoch : 17/25    time : 14s/112s\n",
      "TRAIN    loss : 0.00004    AUC : 1.00000\n",
      "epoch : 18/25    time : 14s/98s\n",
      "TRAIN    loss : 0.00014    AUC : 1.00000\n",
      "epoch : 19/25    time : 14s/84s\n",
      "TRAIN    loss : 0.00006    AUC : 1.00000\n",
      "epoch : 20/25    time : 14s/70s\n",
      "TRAIN    loss : 0.00004    AUC : 1.00000\n",
      "epoch : 21/25    time : 14s/56s\n",
      "TRAIN    loss : 0.00004    AUC : 1.00000\n",
      "epoch : 22/25    time : 14s/42s\n",
      "TRAIN    loss : 0.00002    AUC : 1.00000\n",
      "epoch : 23/25    time : 14s/28s\n",
      "TRAIN    loss : 0.00002    AUC : 1.00000\n",
      "epoch : 24/25    time : 14s/14s\n",
      "TRAIN    loss : 0.00002    AUC : 1.00000\n",
      "epoch : 25/25    time : 14s/0s\n",
      "TRAIN    loss : 0.00003    AUC : 1.00000\n"
     ]
    }
   ],
   "source": [
    "def score_function(real, pred):\n",
    "    score = roc_auc_score(real, pred, average=\"macro\")\n",
    "    return score\n",
    "\n",
    "model = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "\n",
    "\n",
    "best=0\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss = 0\n",
    "    train_pred=[]\n",
    "    train_y=[]\n",
    "    model.train()\n",
    "    for batch in (train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()/len(train_loader)\n",
    "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "        train_y += y.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    \n",
    "    train_auc = score_function(train_y, train_pred)\n",
    "\n",
    "    TIME = time.time() - start\n",
    "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "    print(f'TRAIN    loss : {train_loss:.5f}    AUC : {train_auc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "f_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in (test_loader):\n",
    "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00exusbkgzw1b.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_03dqinf6w0znv.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_046yl0cxn3ybz.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_04athdtx2abyg.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_062aauf9e9jk0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>id_zv5fvjnakvf1r.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>id_zvpikh1z30arn.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>id_zypilwkudljyz.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>id_zz9lwehh5sxdp.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>id_zzq9gaptlwd4w.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Image_id  Label\n",
       "0     id_00exusbkgzw1b.jpg      0\n",
       "1     id_03dqinf6w0znv.jpg      0\n",
       "2     id_046yl0cxn3ybz.jpg      1\n",
       "3     id_04athdtx2abyg.jpg      0\n",
       "4     id_062aauf9e9jk0.jpg      0\n",
       "...                    ...    ...\n",
       "1075  id_zv5fvjnakvf1r.jpg      1\n",
       "1076  id_zvpikh1z30arn.jpg      0\n",
       "1077  id_zypilwkudljyz.jpg      0\n",
       "1078  id_zz9lwehh5sxdp.jpg      1\n",
       "1079  id_zzq9gaptlwd4w.jpg      1\n",
       "\n",
       "[1080 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./zindi/SampleSubmission.csv\")\n",
    "submission[\"Label\"] = f_result\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Submission_v2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set을 사용하지 않고 Train data로만 Efficientnetb0를 학습시켰음에도 test set에 적용한 결과가 매우 뛰어났음\n",
    "### 아마도 이 문제의 train set과 test set의 유사도가 높고, efficientnet의 성능이 워낙 뛰어나다보니 이런 결과가 나타난 것으로 생각됨"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "453dee3dc3901cb9a21ed0866384fb1d6a0d6f4710a79aa791e402cd59be06a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
