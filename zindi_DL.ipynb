{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zindi Image classification (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img # to read images\n",
    "\n",
    "import os                      # to generate new file directories\n",
    "import shutil                  # to move files\n",
    "from tqdm import tqdm          # to see process bar\n",
    "\n",
    "import glob                    # to gather jpg files\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input your own file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your file directory\n",
    "\n",
    "file_dir = 'D:/thon/DL/zindi' # Change here with your file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619\n",
      "               Image_id  Label\n",
      "0  id_02amazy34fgh2.jpg      1\n",
      "1  id_02mh3w48pmyc9.jpg      0\n",
      "2  id_02rpb463h9d3w.jpg      0\n",
      "3  id_02wc3jeeao8ol.jpg      1\n",
      "4  id_03t2hapb8wz8p.jpg      1\n",
      "\n",
      "\n",
      "               Image_id\n",
      "0  id_00exusbkgzw1b.jpg\n",
      "1  id_03dqinf6w0znv.jpg\n",
      "2  id_046yl0cxn3ybz.jpg\n",
      "3  id_04athdtx2abyg.jpg\n",
      "4  id_062aauf9e9jk0.jpg\n",
      "\n",
      "\n",
      "1080\n",
      "1080\n"
     ]
    }
   ],
   "source": [
    "# Read train, test and submission csv files\n",
    "\n",
    "train_df = pd.read_csv(file_dir + '/Train.csv')\n",
    "test_df = pd.read_csv(file_dir + '/Test.csv')\n",
    "sample_sub = pd.read_csv(file_dir + '/SampleSubmission.csv')\n",
    "\n",
    "print(len(train_df['Image_id']))\n",
    "print(train_df.head())\n",
    "print('\\n')\n",
    "print(test_df.head())\n",
    "print('\\n')\n",
    "print(len(test_df['Image_id']))\n",
    "print(np.sum(test_df['Image_id'] == sample_sub['Image_id'])) # Test.csv and SampleSubmission.csv's Image_id is identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check shape of image, since each image has different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1619/1619 [00:22<00:00, 72.64it/s]\n",
      "100%|██████████| 1080/1080 [00:14<00:00, 75.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width   height  channel\n",
      "1024.0  768.0   3.0        526\n",
      "        1024.0  3.0        431\n",
      "        765.0   3.0        237\n",
      "        498.0   3.0        146\n",
      "768.0   1024.0  3.0        110\n",
      "3200.0  2000.0  3.0         59\n",
      "1024.0  640.0   3.0         44\n",
      "498.0   1024.0  3.0         43\n",
      "765.0   1024.0  3.0         18\n",
      "2000.0  3200.0  3.0          3\n",
      "640.0   1024.0  3.0          2\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "width   height  channel\n",
      "1024.0  768.0   3.0        379\n",
      "        1024.0  3.0        263\n",
      "        765.0   3.0        161\n",
      "        498.0   3.0        113\n",
      "768.0   1024.0  3.0         73\n",
      "3200.0  2000.0  3.0         33\n",
      "1024.0  640.0   3.0         21\n",
      "498.0   1024.0  3.0         18\n",
      "765.0   1024.0  3.0         14\n",
      "2000.0  3200.0  3.0          4\n",
      "640.0   1024.0  3.0          1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### To check the shape of whole images\n",
    "\n",
    "train_df['width'] = np.nan\n",
    "train_df['height'] = np.nan\n",
    "train_df['channel'] = np.nan\n",
    "\n",
    "test_df['width'] = np.nan\n",
    "test_df['height'] = np.nan\n",
    "test_df['channel'] = np.nan\n",
    "\n",
    "\n",
    "# Set image directory\n",
    "\n",
    "img_dir = file_dir + '/Images'\n",
    "\n",
    "# To check train images' shape\n",
    "\n",
    "for i in tqdm(range(len(train_df))):\n",
    "  image = img.imread(img_dir + '/' + train_df['Image_id'].loc[i])\n",
    "  w, h, c = image.shape\n",
    "  train_df.loc[i, 'width'] = w\n",
    "  train_df.loc[i, 'height'] = h\n",
    "  train_df.loc[i, 'channel'] = c\n",
    "\n",
    "# To check test images' shape\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "  image = img.imread(img_dir + '/' + test_df['Image_id'].loc[i])\n",
    "  w, h, c = image.shape\n",
    "  test_df.loc[i, 'width'] = w\n",
    "  test_df.loc[i, 'height'] = h\n",
    "  test_df.loc[i, 'channel'] = c\n",
    "\n",
    "\n",
    "# Train image's shape\n",
    "\n",
    "print(train_df[['width', 'height', 'channel']].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "# Test images' shape\n",
    "\n",
    "print(test_df[['width', 'height', 'channel']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Image directories and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You don't need to do it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't need to do it twice\n",
    "\n",
    "# Generate new 'train', 'test' image folders in your file_directory\n",
    "\n",
    "os.makedirs(img_dir + '/train')\n",
    "os.makedirs(img_dir + '/test')\n",
    "\n",
    "# Set train and test directories\n",
    "\n",
    "train_dir = img_dir + '/train'\n",
    "test_dir = img_dir + '/test'\n",
    "\n",
    "# Generate '0' / '1' label folder in 'train' folder\n",
    "\n",
    "os.makedirs(train_dir + '/1')\n",
    "os.makedirs(train_dir + '/0')\n",
    "\n",
    "# Set train label 0 and 1 directories\n",
    "\n",
    "train1_dir = train_dir + '/1'\n",
    "train0_dir = train_dir + '/0'\n",
    "\n",
    "\n",
    "### To move files according to train.csv and test.csv's 'Image_id' column\n",
    "\n",
    "train_file_name = train_df[\"Image_id\"]\n",
    "train_file_label = train_df[\"Label\"]\n",
    "test_file_name = test_df[\"Image_id\"]\n",
    "\n",
    "# Move train images from 'Images' folder to 'train' folder\n",
    "\n",
    "for i in train_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = train_dir\n",
    "    shutil.move(file_source, file_destination)\n",
    "\n",
    "\n",
    "# Move test images from 'Images' folder to 'test' folder\n",
    "\n",
    "for i in test_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = test_dir\n",
    "    shutil.move(file_source, file_destination)\n",
    "\n",
    "\n",
    "# Separate train images to '0' / '1' label folder\n",
    "\n",
    "for i, name in enumerate(train_file_name):\n",
    "    file_source = train_dir + f'/{name}'\n",
    "    file_destination_1 = train1_dir # for Label 1\n",
    "    file_destination_0 = train0_dir # for Label 0\n",
    "\n",
    "    if train_file_label[i] == 1:\n",
    "        shutil.move(file_source, file_destination_1)\n",
    "    else:\n",
    "        shutil.move(file_source, file_destination_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1619 files belonging to 2 classes.\n",
      "Using 1296 files for training.\n",
      "\n",
      "\n",
      "Found 1619 files belonging to 2 classes.\n",
      "Using 323 files for validation.\n",
      "\n",
      "\n",
      "Found 1080 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "### Image preprocessing\n",
    "# Set batch_size, img_height, img_width\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "\n",
    "# Train image dataset\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split = 0.2, # 1619 * 0.8 = 1296 images are for train dataset\n",
    "    label_mode = 'binary',\n",
    "    subset = 'training',\n",
    "    shuffle = True,\n",
    "    seed = 2021120087,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Validation image dataset\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split = 0.2, # 1619 * 0.2 = 323 images are for validation dataset\n",
    "    label_mode = 'binary',\n",
    "    subset = \"validation\",\n",
    "    shuffle = True,\n",
    "    seed = 2021120087,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Test image dataset\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode = None,\n",
    "    shuffle = False,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "(32, 180, 180, 3)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Class names (Labels)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "\n",
    "# Check the train image shapes\n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization (Not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\thon\\DL\\zindi_DL.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_DL.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39m# Set normalization layer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_DL.ipynb#ch0000014?line=2'>3</a>\u001b[0m normalization_layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mRescaling(\u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_DL.ipynb#ch0000014?line=5'>6</a>\u001b[0m \u001b[39m# Normalization train_ds\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/thon/DL/zindi_DL.ipynb#ch0000014?line=7'>8</a>\u001b[0m normalized_train_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (normalization_layer(x), y))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Set normalization layer\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255)\n",
    "\n",
    "\n",
    "# Normalization train_ds\n",
    "\n",
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_ds))\n",
    "\n",
    "# Check how well normalization is applied\n",
    "print(np.min(image_batch), np.max(image_batch))\n",
    "\n",
    "# Normalization for test_ds\n",
    "\n",
    "normalized_test_ds = test_ds.map(lambda x: (normalization_layer(x)))\n",
    "test_image_batch = next(iter(normalized_test_ds))\n",
    "\n",
    "print(np.min(test_image_batch), np.max(test_image_batch))\n",
    "\n",
    "### Normalized dataset had lower accuracy for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
      "_________________________________________________________________\n",
      "rescaling_2 (Rescaling)      (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 178, 178, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 89, 89, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 87, 87, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 12545     \n",
      "=================================================================\n",
      "Total params: 991,041\n",
      "Trainable params: 991,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ConvNet layers\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape = (180, 180, 3))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 128, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 256, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 256, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "# Model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.losses.BinaryCrossentropy(),\n",
    "    metrics = ['Accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = 20, verbose = 0, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize validation loss and accuracy loss\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label = \"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/41 [==============================] - 5s 95ms/step - loss: 0.6567 - auc: 0.6537 - val_loss: 0.6228 - val_auc: 0.8502\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.5060 - auc: 0.8363 - val_loss: 0.4167 - val_auc: 0.8943\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.4145 - auc: 0.8975 - val_loss: 0.3216 - val_auc: 0.9382\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 0.2874 - auc: 0.9493 - val_loss: 0.2627 - val_auc: 0.9662\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.2172 - auc: 0.9709 - val_loss: 0.2686 - val_auc: 0.9531\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 0.1767 - auc: 0.9805 - val_loss: 0.1984 - val_auc: 0.9843\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.1405 - auc: 0.9871 - val_loss: 0.1855 - val_auc: 0.9794\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.1104 - auc: 0.9928 - val_loss: 0.1548 - val_auc: 0.9885\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0772 - auc: 0.9959 - val_loss: 0.1604 - val_auc: 0.9876\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0770 - auc: 0.9967 - val_loss: 0.1489 - val_auc: 0.9875\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0476 - auc: 0.9988 - val_loss: 0.1702 - val_auc: 0.9850\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0646 - auc: 0.9976 - val_loss: 0.1706 - val_auc: 0.9867\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 0.0270 - auc: 0.9997 - val_loss: 0.1537 - val_auc: 0.9869\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0178 - auc: 0.9999 - val_loss: 0.1762 - val_auc: 0.9847\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 0.0083 - auc: 1.0000 - val_loss: 0.2074 - val_auc: 0.9789\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 0.0332 - auc: 0.9993 - val_loss: 0.1966 - val_auc: 0.9789\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 0.0094 - auc: 1.0000 - val_loss: 0.1962 - val_auc: 0.9808\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 0.0031 - auc: 1.0000 - val_loss: 0.2345 - val_auc: 0.9750\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 8.1195e-04 - auc: 1.0000 - val_loss: 0.2372 - val_auc: 0.9781\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 4.8156e-04 - auc: 1.0000 - val_loss: 0.2527 - val_auc: 0.9778\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 2.5683e-04 - auc: 1.0000 - val_loss: 0.2509 - val_auc: 0.9783\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 1.9915e-04 - auc: 1.0000 - val_loss: 0.2589 - val_auc: 0.9760\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 4s 96ms/step - loss: 1.5207e-04 - auc: 1.0000 - val_loss: 0.2663 - val_auc: 0.9735\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 1.1980e-04 - auc: 1.0000 - val_loss: 0.2739 - val_auc: 0.9735\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 9.7082e-05 - auc: 1.0000 - val_loss: 0.2834 - val_auc: 0.9734\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 8.1829e-05 - auc: 1.0000 - val_loss: 0.2933 - val_auc: 0.9736\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 6.8242e-05 - auc: 1.0000 - val_loss: 0.2964 - val_auc: 0.9706\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 5.7005e-05 - auc: 1.0000 - val_loss: 0.3092 - val_auc: 0.9738\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 5.3676e-05 - auc: 1.0000 - val_loss: 0.3099 - val_auc: 0.9709\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 4.3339e-05 - auc: 1.0000 - val_loss: 0.3207 - val_auc: 0.9712\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 4.4777e-05 - auc: 1.0000 - val_loss: 0.3237 - val_auc: 0.9711\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 3.6875e-05 - auc: 1.0000 - val_loss: 0.3334 - val_auc: 0.9711\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 3.7643e-05 - auc: 1.0000 - val_loss: 0.3310 - val_auc: 0.9711\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 2.9721e-05 - auc: 1.0000 - val_loss: 0.3356 - val_auc: 0.9711\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 2.5816e-05 - auc: 1.0000 - val_loss: 0.3368 - val_auc: 0.9711\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 4s 95ms/step - loss: 2.2278e-05 - auc: 1.0000 - val_loss: 0.3476 - val_auc: 0.9686\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 2.1186e-05 - auc: 1.0000 - val_loss: 0.3517 - val_auc: 0.9712\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.8822e-05 - auc: 1.0000 - val_loss: 0.3552 - val_auc: 0.9686\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.6891e-05 - auc: 1.0000 - val_loss: 0.3603 - val_auc: 0.9686\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.5348e-05 - auc: 1.0000 - val_loss: 0.3659 - val_auc: 0.9686\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 1.4932e-05 - auc: 1.0000 - val_loss: 0.3721 - val_auc: 0.9658\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.4576e-05 - auc: 1.0000 - val_loss: 0.3694 - val_auc: 0.9686\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.1574e-05 - auc: 1.0000 - val_loss: 0.3747 - val_auc: 0.9686\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 1.0128e-05 - auc: 1.0000 - val_loss: 0.3828 - val_auc: 0.9659\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 9.9060e-06 - auc: 1.0000 - val_loss: 0.3752 - val_auc: 0.9686\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 9.0840e-06 - auc: 1.0000 - val_loss: 0.3897 - val_auc: 0.9658\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 8.2325e-06 - auc: 1.0000 - val_loss: 0.3937 - val_auc: 0.9659\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 7.3322e-06 - auc: 1.0000 - val_loss: 0.3946 - val_auc: 0.9659\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 4s 92ms/step - loss: 6.3875e-06 - auc: 1.0000 - val_loss: 0.4034 - val_auc: 0.9661\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 5.8387e-06 - auc: 1.0000 - val_loss: 0.4148 - val_auc: 0.9634\n"
     ]
    }
   ],
   "source": [
    "### Find the best model using callbacks\n",
    "# Define model again\n",
    "\n",
    "inputs = keras.Input(shape = (180, 180, 3))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 128, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 256, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = layers.Conv2D(filters = 256, kernel_size = 3, activation = \"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.losses.BinaryCrossentropy(),\n",
    "    metrics = ['Accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = \"convnet.keras\", # Save the best model as 'convnet.kears'\n",
    "        save_best_only = True,\n",
    "        monitor = \"val_loss\")\n",
    "]\n",
    "\n",
    "history_test = model.fit(\n",
    "    train_ds,\n",
    "    epochs = 50,\n",
    "    validation_data = val_ds,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00exusbkgzw1b.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_03dqinf6w0znv.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_046yl0cxn3ybz.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_04athdtx2abyg.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_062aauf9e9jk0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image_id  Label\n",
       "0  id_00exusbkgzw1b.jpg      0\n",
       "1  id_03dqinf6w0znv.jpg      0\n",
       "2  id_046yl0cxn3ybz.jpg      1\n",
       "3  id_04athdtx2abyg.jpg      0\n",
       "4  id_062aauf9e9jk0.jpg      0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the best model\n",
    "\n",
    "test_model = keras.models.load_model(\"convnet.keras\") # load the best model which we defined \"convnet.keras\"\n",
    "\n",
    "predictions = test_model.predict(test_ds)\n",
    "prediction_classes = [\n",
    "    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
    "]\n",
    "\n",
    "sample_sub[\"Label\"] = prediction_classes\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv file\n",
    "\n",
    "sample_sub.to_csv(\"Submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "453dee3dc3901cb9a21ed0866384fb1d6a0d6f4710a79aa791e402cd59be06a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
