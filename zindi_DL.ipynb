{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zindi Image classification (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from tqdm import tqdm   # to see process status bar\n",
    "import os  # to generate new directories\n",
    "import glob # to gather jpg files\n",
    "from glob import glob\n",
    "import shutil  # to move files\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Image_id  Label\n",
      "0  id_02amazy34fgh2.jpg      1\n",
      "1  id_02mh3w48pmyc9.jpg      0\n",
      "2  id_02rpb463h9d3w.jpg      0\n",
      "3  id_02wc3jeeao8ol.jpg      1\n",
      "4  id_03t2hapb8wz8p.jpg      1\n",
      "\n",
      "\n",
      "               Image_id\n",
      "0  id_00exusbkgzw1b.jpg\n",
      "1  id_03dqinf6w0znv.jpg\n",
      "2  id_046yl0cxn3ybz.jpg\n",
      "3  id_04athdtx2abyg.jpg\n",
      "4  id_062aauf9e9jk0.jpg\n",
      "\n",
      "\n",
      "               Image_id  Label\n",
      "0  id_00exusbkgzw1b.jpg      0\n",
      "1  id_03dqinf6w0znv.jpg      0\n",
      "2  id_046yl0cxn3ybz.jpg      0\n",
      "3  id_04athdtx2abyg.jpg      0\n",
      "4  id_062aauf9e9jk0.jpg      0\n"
     ]
    }
   ],
   "source": [
    "# Input your file directory\n",
    "\n",
    "file_dir = 'D:/thon/DL/zindi' # Change here with your file directory\n",
    "\n",
    "# Read train, test and submission csv files\n",
    "\n",
    "train_df = pd.read_csv(file_dir + '/Train.csv')\n",
    "test_df = pd.read_csv(file_dir + '/Test.csv')\n",
    "sample_sub = pd.read_csv(file_dir + '/SampleSubmission.csv')\n",
    "\n",
    "print(train_df.head())\n",
    "print('\\n')\n",
    "print(test_df.head())\n",
    "print('\\n')\n",
    "print(sample_sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check shape of image, since each image has different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1619/1619 [00:22<00:00, 72.64it/s]\n",
      "100%|██████████| 1080/1080 [00:14<00:00, 75.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width   height  channel\n",
      "1024.0  768.0   3.0        526\n",
      "        1024.0  3.0        431\n",
      "        765.0   3.0        237\n",
      "        498.0   3.0        146\n",
      "768.0   1024.0  3.0        110\n",
      "3200.0  2000.0  3.0         59\n",
      "1024.0  640.0   3.0         44\n",
      "498.0   1024.0  3.0         43\n",
      "765.0   1024.0  3.0         18\n",
      "2000.0  3200.0  3.0          3\n",
      "640.0   1024.0  3.0          2\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "width   height  channel\n",
      "1024.0  768.0   3.0        379\n",
      "        1024.0  3.0        263\n",
      "        765.0   3.0        161\n",
      "        498.0   3.0        113\n",
      "768.0   1024.0  3.0         73\n",
      "3200.0  2000.0  3.0         33\n",
      "1024.0  640.0   3.0         21\n",
      "498.0   1024.0  3.0         18\n",
      "765.0   1024.0  3.0         14\n",
      "2000.0  3200.0  3.0          4\n",
      "640.0   1024.0  3.0          1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### To check the shape of whole images\n",
    "\n",
    "train_df['width'] = np.nan\n",
    "train_df['height'] = np.nan\n",
    "train_df['channel'] = np.nan\n",
    "\n",
    "test_df['width'] = np.nan\n",
    "test_df['height'] = np.nan\n",
    "test_df['channel'] = np.nan\n",
    "\n",
    "\n",
    "# Set image directory\n",
    "\n",
    "img_dir = file_dir + '/Images'\n",
    "\n",
    "# To check train images' shape\n",
    "\n",
    "for i in tqdm(range(len(train_df))):\n",
    "  image = img.imread(img_dir + '/' + train_df['Image_id'].loc[i])\n",
    "  w, h, c = image.shape\n",
    "  train_df.loc[i, 'width'] = w\n",
    "  train_df.loc[i, 'height'] = h\n",
    "  train_df.loc[i, 'channel'] = c\n",
    "\n",
    "# To check test images' shape\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "  image = img.imread(img_dir + '/' + test_df['Image_id'].loc[i])\n",
    "  w, h, c = image.shape\n",
    "  test_df.loc[i, 'width'] = w\n",
    "  test_df.loc[i, 'height'] = h\n",
    "  test_df.loc[i, 'channel'] = c\n",
    "\n",
    "\n",
    "# Train image's shape\n",
    "\n",
    "print(train_df[['width', 'height', 'channel']].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "# Test images' shape\n",
    "\n",
    "print(test_df[['width', 'height', 'channel']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Image directories and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You don't need to do it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't need to do it twice\n",
    "\n",
    "# Generate new 'train', 'test' image folders in your file_directory\n",
    "\n",
    "os.makedirs(img_dir + '/train')\n",
    "os.makedirs(img_dir + '/test')\n",
    "\n",
    "# Set train and test directories\n",
    "\n",
    "train_dir = img_dir + '/train'\n",
    "test_dir = img_dir + '/test'\n",
    "\n",
    "# Generate '0' / '1' label folder in 'train' folder\n",
    "\n",
    "os.makedirs(train_dir + '/1')\n",
    "os.makedirs(train_dir + '/0')\n",
    "\n",
    "# Set train label 0 and 1 directories\n",
    "\n",
    "train1_dir = train_dir + '/1'\n",
    "train0_dir = train_dir + '/0'\n",
    "\n",
    "\n",
    "### To move files according to train.csv and test.csv's 'Image_id' column\n",
    "\n",
    "train_file_name = train_df[\"Image_id\"]\n",
    "train_file_label = train_df[\"Label\"]\n",
    "test_file_name = test_df[\"Image_id\"]\n",
    "\n",
    "# Move train images from 'Images' folder to 'train' folder\n",
    "\n",
    "for i in train_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = train_dir\n",
    "    shutil.move(file_source, file_destination)\n",
    "\n",
    "\n",
    "# Move test images from 'Images' folder to 'test' folder\n",
    "\n",
    "for i in test_file_name:\n",
    "    file_source = img_dir + f'/{i}'\n",
    "    file_destination = test_dir\n",
    "    shutil.move(file_source, file_destination)\n",
    "\n",
    "\n",
    "# Separate train images to '0' / '1' label folder\n",
    "\n",
    "for i, name in enumerate(train_file_name):\n",
    "    file_source = train_dir + f'/{name}'\n",
    "    file_destination_1 = train1_dir # for Label 1\n",
    "    file_destination_0 = train0_dir # for Label 0\n",
    "\n",
    "    if train_file_label[i] == 1:\n",
    "        shutil.move(file_source, file_destination_1)\n",
    "    else:\n",
    "        shutil.move(file_source, file_destination_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1619 files belonging to 2 classes.\n",
      "Using 1296 files for training.\n",
      "\n",
      "\n",
      "Found 1619 files belonging to 2 classes.\n",
      "Using 323 files for validation.\n",
      "\n",
      "\n",
      "Found 1080 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "### Image preprocessing\n",
    "# Set batch_size, img_height, img_width\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "\n",
    "# Train image dataset\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split = 0.2, # 1619 * 0.8 = 1296 images are for train dataset\n",
    "    label_mode = 'binary',\n",
    "    subset = 'training',\n",
    "    shuffle = True,\n",
    "    seed = 2021120087,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Validation image dataset\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split = 0.2, # 1619 * 0.2 = 323 images are for validation dataset\n",
    "    label_mode = 'binary',\n",
    "    subset = \"validation\",\n",
    "    shuffle = True,\n",
    "    seed = 2021120087,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Test image dataset\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode = None,\n",
    "    shuffle = False,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "(32, 180, 180, 3)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Class names (Labels)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "\n",
    "# Check the train image shapes\n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Set normalization layer\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255)\n",
    "\n",
    "\n",
    "# Min-Max Normalization train_ds\n",
    "\n",
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_ds))\n",
    "\n",
    "# Check how well min-max normalization is applied\n",
    "print(np.min(image_batch), np.max(image_batch))\n",
    "\n",
    "# Min-Max Normalization for test_ds\n",
    "\n",
    "normalized_test_ds = test_ds.map(lambda x: (normalization_layer(x)))\n",
    "test_image_batch = next(iter(normalized_test_ds))\n",
    "\n",
    "print(np.min(test_image_batch), np.max(test_image_batch))\n",
    "\n",
    "### Question) Isn't Normalization necessary for validation set???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I think this is not necessary\n",
    "# Maintain images caches in memory\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
      "_________________________________________________________________\n",
      "rescaling_1 (Rescaling)      (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 178, 178, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 87, 87, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 12545     \n",
      "=================================================================\n",
      "Total params: 991,041\n",
      "Trainable params: 991,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model ConvNet\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.losses.BinaryCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = 20, verbose = 0, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize validation loss and accuracy loss\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/41 [==============================] - 5s 92ms/step - loss: 0.0498 - accuracy: 0.9799 - val_loss: 0.2324 - val_accuracy: 0.9350\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.9381\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 7.4357e-04 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.9412\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 2.8264e-04 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 0.9412\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 1.6230e-04 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 0.9412\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 9.7415e-05 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 0.9412\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 4s 91ms/step - loss: 9.2353e-05 - accuracy: 1.0000 - val_loss: 0.2869 - val_accuracy: 0.9412\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 4.8684e-05 - accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9412\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 4s 88ms/step - loss: 3.4408e-05 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 0.9443\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 2.2184e-05 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9381\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.9539e-05 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 0.9443\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 1.7596e-05 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9412\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 1.1435e-05 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 0.9412\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 9.2845e-06 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9412\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 4s 93ms/step - loss: 6.7228e-06 - accuracy: 1.0000 - val_loss: 0.3812 - val_accuracy: 0.9412\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 5.4919e-06 - accuracy: 1.0000 - val_loss: 0.3882 - val_accuracy: 0.9412\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 3.6027e-06 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 0.9412\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.9979e-06 - accuracy: 1.0000 - val_loss: 0.4086 - val_accuracy: 0.9412\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 2.6042e-06 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9412\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.2147e-06 - accuracy: 1.0000 - val_loss: 0.4206 - val_accuracy: 0.9412\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 2.0540e-06 - accuracy: 1.0000 - val_loss: 0.4264 - val_accuracy: 0.9412\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 1.6590e-06 - accuracy: 1.0000 - val_loss: 0.4299 - val_accuracy: 0.9412\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 1.5877e-06 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9412\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 4s 88ms/step - loss: 1.5636e-06 - accuracy: 1.0000 - val_loss: 0.4402 - val_accuracy: 0.9412\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 4s 87ms/step - loss: 1.2550e-06 - accuracy: 1.0000 - val_loss: 0.4426 - val_accuracy: 0.9443\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 1.4395e-06 - accuracy: 1.0000 - val_loss: 0.4462 - val_accuracy: 0.9443\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 1.0810e-06 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9443\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 9.0072e-07 - accuracy: 1.0000 - val_loss: 0.4569 - val_accuracy: 0.9443\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 7.9019e-07 - accuracy: 1.0000 - val_loss: 0.4630 - val_accuracy: 0.9443\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 7.0741e-07 - accuracy: 1.0000 - val_loss: 0.4676 - val_accuracy: 0.9443\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 6.7897e-07 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.9412\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 4s 90ms/step - loss: 6.3951e-07 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.9412\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 5.6148e-07 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.9412\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 4.7888e-07 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.9443\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 4.7419e-07 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9443\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 4.1983e-07 - accuracy: 1.0000 - val_loss: 0.4893 - val_accuracy: 0.9443\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 3.8747e-07 - accuracy: 1.0000 - val_loss: 0.4943 - val_accuracy: 0.9443\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 3.6492e-07 - accuracy: 1.0000 - val_loss: 0.4947 - val_accuracy: 0.9443\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 3.4995e-07 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.9412\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 3.0300e-07 - accuracy: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.9443\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 3.0262e-07 - accuracy: 1.0000 - val_loss: 0.5057 - val_accuracy: 0.9443\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.8467e-07 - accuracy: 1.0000 - val_loss: 0.5080 - val_accuracy: 0.9443\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.5147e-07 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.9443\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 2.3054e-07 - accuracy: 1.0000 - val_loss: 0.5123 - val_accuracy: 0.9443\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 4s 89ms/step - loss: 2.4989e-07 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.9443\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.2110e-07 - accuracy: 1.0000 - val_loss: 0.5180 - val_accuracy: 0.9443\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 2.0308e-07 - accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.9443\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 1.8063e-07 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 0.9443\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 4s 86ms/step - loss: 1.7514e-07 - accuracy: 1.0000 - val_loss: 0.5252 - val_accuracy: 0.9443\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 4s 85ms/step - loss: 1.5357e-07 - accuracy: 1.0000 - val_loss: 0.5258 - val_accuracy: 0.9443\n"
     ]
    }
   ],
   "source": [
    "# Find the best model using callbacks\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = tf.losses.BinaryCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history_test = model.fit(\n",
    "    train_ds,\n",
    "    epochs = 50,\n",
    "    validation_data = val_ds,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00exusbkgzw1b.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_03dqinf6w0znv.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_046yl0cxn3ybz.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_04athdtx2abyg.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_062aauf9e9jk0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image_id  Label\n",
       "0  id_00exusbkgzw1b.jpg      0\n",
       "1  id_03dqinf6w0znv.jpg      0\n",
       "2  id_046yl0cxn3ybz.jpg      1\n",
       "3  id_04athdtx2abyg.jpg      0\n",
       "4  id_062aauf9e9jk0.jpg      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the best model\n",
    "\n",
    "test_model = keras.models.load_model(\n",
    "    \"convnet_from_scratch_with_augmentation.keras\")\n",
    "\n",
    "predictions = test_model.predict(test_ds)\n",
    "prediction_classes = [\n",
    "    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
    "]\n",
    "\n",
    "sample_sub[\"Label\"] = prediction_classes\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv file\n",
    "\n",
    "sample_sub.to_csv(\"SampleSubmission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "453dee3dc3901cb9a21ed0866384fb1d6a0d6f4710a79aa791e402cd59be06a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
